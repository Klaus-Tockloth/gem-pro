# --------------------------------------------------
#
# Purpose:
# - Program configuration for Gemini AI Client
# 
# Release:
# - v0.14 - 2026-02-05: corresponding to program release
#
# Remarks:
# - Do not use tabs or unnecessary white spaces in YAML files.
# - !!str = indicates that the associated value is a string
# - Not all options are applicable to all Gemini models.
#
# Useful links:
# - https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters
# --------------------------------------------------

# Gemini configuration section
# ----------------------------

# Gemini API key (your private key, don't share)
# 'env:var': obtain api-key from environment variable
# 'file:pathname': first line of pathname is api-key
# 'pass:api-key': pass contains the api-key
GeminiAPIKey: 'env:GEMINI_API_KEY'

# Gemini AI text model family (options: -lite, -flash, -pro)
GeminiLiteAiModel: models/gemini-2.5-flash-lite-preview-09-2025
GeminiFlashAiModel: models/gemini-3-flash-preview
GeminiProAiModel: models/gemini-3-pro-preview

# Gemini AI image model family (options: -flash-image [Nano Banana], -pro-image [Nano Banana Pro])
GeminiFlashImageAiModel: models/gemini-2.5-flash-image
GeminiProImageAiModel: models/gemini-3-pro-image-preview

# Gemini AI default model
GeminiDefaultAiModel: models/gemini-3-pro-image-preview

# list of supported response modalities (e.g. TEXT and IMAGE)
GeminiResponseModalities:
- TEXT
- IMAGE

# number of generated responses to return (int)
GeminiCandidateCount: 1

# pure response without any boilerplate
GeminiPureResponse: false

# Image Generation: AspectRatio: '1:1', '2:3', '3:2', '3:4', '4:3', '4:5', '5:4', '9:16', '16:9', or '21:9'
# Note: Important technical setting not possible within the prompt.
# Square
#   1:1: Best for profile pictures (avatars), Instagram grid posts, and logo design. It is the most neutral format for general subjects.
# Vertical (Portrait)
#   9:16: The standard for full-screen mobile content. Use this for TikTok, Instagram Reels, YouTube Shorts, and mobile wallpapers.
#   2:3: Classic photography ratio (35mm film rotated). Great for portrait photography and Pinterest pins.
#   3:4: Common in digital cameras and iPads. Good for portraits that need a bit more width than 2:3.
#   4:5: The optimized vertical format for Instagram Feed posts. It takes up the maximum amount of screen space.
# Horizontal (Landscape)
#   16:9: The standard for YouTube videos, television, monitors, and presentation slides.
#   3:2: Classic 35mm photography ratio. The standard for landscape photography and DSLR sensors.
#   4:3: Traditional monitor shape and standard digital camera sensor. Great for general illustrations and classic TV style.
#   5:4: A slightly taller landscape, often used in classic large-format art prints (like 8x10 or 16x20 inches).
#   21:9: Ultrawide or Cinematic. Use this for movie concept art, gaming wallpapers, or wide panoramic landscapes.
GeminiImageAspectRatio: "4:3"

# Image Generation: Resolution/ImageSize: "1K", "2K", "4K"
# Note: Important technical setting not possible within the prompt.
# 1K (Standard / Low usage): Rapid prototyping, testing prompts, or thumbnail creation.
# 2K (High Definition): General purpose web use, digital art portfolios, and standard screen viewing.
# 4K (Ultra High Definition): Printing, desktop wallpapers, professional post-production, or highly detailed illustrations.
GeminiImageResolution: "2K"

# control the randomness of the output (varies by model, float)
# Values can range over [0.0, MaxTemperature], inclusive. A higher value will produce responses that
# are more varied, while a value closer to 0.0 will typically result in less surprising responses
# from the model.
# Important for Gemini 3 (and Reasoning Models):
# Google strongly recommends keeping the temperature parameter at its default value (usually 1.0).
# Changing the temperature (especially setting it below 1.0) may lead to unexpected behavior, 
# such as looping or degraded performance in reasoning tasks.
GeminiTemperature:

# maximum cumulative probability of tokens to consider when sampling (float)
# The topP parameter changes how the model selects tokens for output.
# Important for Gemini 3
# It is recommended to keep this at the default value (typically 0.95).
GeminiTopP:

# maximum number of tokens to consider when sampling (int)
# The topK parameter changes how the model selects tokens for output.
GeminiTopK:

# maximum number of tokens to include in a candidate (int)
# Specifies the maximum number of tokens that can be generated in the response. A token 
# is approximately four characters. 100 tokens correspond to roughly 60-80 english words.
GeminiMaxOutputTokens:

# SystemInstruction (also known as "system prompt") is a more forceful prompt to the model.
# The model will adhere the instructions more strongly than if they appeared in a normal prompt.
# Typical usage: Optional tone and style instructions for the model.
# Examples:
# - 'Provide raw markdown only, no conversational filler, no wrapping code blocks.
#    Ensure LaTeX math blocks ($$) are always placed on their own new lines.'
# - 'Translate prompt from English to German. Pay attention to correct grammar and a fluent style.'
# - 'Use the technical terms common in the subject area. Avoid filler words.'
# - 'All questions should be answered comprehensively with details, unless the user requests a
#    concise response specifically. Respond in the same language as the query.'
# - 'Answer as concisely as possible.'
# - 'Don't use technical terms in your response.'
# System instructions let you steer the behavior of a model based on your specific needs and use cases.
# By giving the model system instructions, you provide the model additional context to understand the
# task, generate more customized responses, and adhere to specific guidelines over the full user interaction 
# with the model. System instructions are provided via text file (e.g. gemini-system-instruction.txt).
GeminiSystemInstruction: gemini-system-instruction.txt

# ground response allowing the AI model to generate and execute program code (in a sandbox)
GeminiGroundingWithCodeExecution: false

# ground responses with Google Search (not supported by all AI models, affects other settings)
GeminiGroundingWithGoogleSearch: true

# ground responses using specific URLs provided in the prompt
GeminiGroundingWithURLContext: false

# ground responses with Google Maps (not supported by all AI models, affects other settings)
GeminiGroundigWithGoogleMaps: false

# ground responses with user defined FileSearchStores (RAG, Retrieval-Augmented Generation)
GeminiGroundingWithFileSearchStores:
# - fileSearchStores/demo-store-0no9k650jg7w
# - fileSearchStores/my-knowledge-57qmn5qr3qt5
# - ...

# max thinking budget (AI model must support this setting, -1 = dynamic thinking, 0 = no thinking)
# Lite, Flash : -1, 0-24576 (24576 = use as much budget as possible)
# Pro         : -1, 128-32768 (32768 = use as much budget as possible)
# Warning: Unlimited thinking can result in high prompt costs.
# Info: Deprecated for 'Gemini 3'. Use thinking level instead.
GeminiMaxThinkingBudget:

# level of thoughts tokens that the model should generate (minimal, low, medium or high)
GeminiThinkingLevel:

# include thoughts in the response (true, false) (AI model must support this setting)
GeminiIncludeThoughts:

# (logical) name of cache handled by this application (note: only one cache can be added to a prompt)
# This logical cache name is used for all cache operations (create, list, delete, include).
# Caching must be supported by AI model.
GeminiCacheName: gem-pro-cache

# time to live (ttl) for cached items in hours
# Keep in mind that caching objects has costs.
GeminiCacheTimeToLive: 1

# input media resolution (low, medium, high) affects images, videos and pdfs
# recommended settings: image=high, video=low/medium, pdf=medium
# Note: Affects only INPUT files. 
GeminiInputMediaResolution:

# Markdown rendering section
# --------------------------

# handling of current prompt/response pair (valid filename is mandatory)
# MarkdownRendering: not configurable, the native AI response has already markdown formatting
MarkdownPromptResponseFile: prompt-response.md

# output of current prompt/response pair (%s = placeholder for name of file)
MarkdownOutput: false
MarkdownOutputApplicationMacOS: 'open -a "/Applications/Markdown Editor.app" %s'
MarkdownOutputApplicationLinux:
MarkdownOutputApplicationWindows:
MarkdownOutputApplicationOther:

# copy each prompt/response file to history (schema = yyyymmdd-hhmmss.html)
MarkdownHistory: true
MarkdownHistoryDirectory: ./history-markdown

# Ansi (terminal) rendering section
# ---------------------------------

# handling of current prompt/response pair
AnsiRendering: true
AnsiPromptResponseFile: prompt-response.ansi

# output of current prompt/response pair
AnsiOutput: true

# output line length
# note: make your terminal wider to avoid unwanted line breaks
AnsiOutputLineLength: 132

# theme (glamour style) for ansi output
# glamour predefined: dark, dracula, light, notty, pink, tokyo-night
# custom predefined: ./assets/glamour-dark-gem-pro.json, ./assets/glamour-light-gem-pro.json
# custom: create your own glamour style and reference the file path
#         see https://github.com/charmbracelet/glamour/tree/master/styles
AnsiOutputTheme: ./assets/glamour-dark-gem-pro.json

# copy each prompt/response file to history (schema = yyyymmdd-hhmmss.html)
AnsiHistory: true
AnsiHistoryDirectory: ./history-ansi

# HTML rendering section
# ----------------------

# handling of current prompt/response pair
HTMLRendering: true
HTMLPromptResponseFile: prompt-response.html

# output of current prompt/response pair (%s = placeholder for name of file)
# it is also possible to specify the browser (e.g. 'open -a "Google Chrome" %s')
HTMLOutput: true
HTMLOutputApplicationMacOS: 'open %s'
HTMLOutputApplicationLinux: 'xdg-open %s'
HTMLOutputApplicationWindows: 'cmd /c start "" %s'
HTMLOutputApplicationOther:

# copy each prompt/response file to history (schema = yyyymmdd-hhmmss.html)
HTMLHistory: true
HTMLHistoryDirectory: ./history-html

# maximum length of webpage title (equal with first n characters of prompt)
HTMLMaxLengthTitle: 200

# list of HTML elements to replace or remove (modifies HTML output)
HTMLReplaceElements:
- 'class="language-mermaid"': 'class="mermaid"'

# header to insert at beginning of html page (do not change title, %s is placeholder for prompt)
HTMLHeader: |
  <!DOCTYPE html>
  <head>
    <meta charset="UTF-8">
    <title>%s</title>
    <link rel="icon" type="image/svg+xml" href="assets/gemini-prompt-303030.svg" media="(prefers-color-scheme: light)">
    <link rel="icon" type="image/svg+xml" href="assets/gemini-prompt-ebebeb.svg" media="(prefers-color-scheme: dark)">
    <!-- replace highlight theme with your favorite one -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-light.min.css" media="(prefers-color-scheme: light)">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.min.css" media="(prefers-color-scheme: dark)">
    <link rel="stylesheet" type="text/css" href="assets/gemini-prompt.css">
  </head>
  <body>

# footer to add to end of html page (e.g. to add javascript functionality)
HTMLFooter: |
  <!-- highlight: syntax highlighter with many themes -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <!-- add 'copy to clipboard' button to all '<pre><code>' block elements -->
  <script src="assets/copy-to-clipboard.js"></script>
  <!-- mathjax: adds both block math and inline math support -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,  // allows \$
        processEnvironments: true
      },
      options: {
        ignoreHtmlClass: 'tex2jax_ignore',
        processHtmlClass: 'tex2jax_process'
      }
    };
  </script>
  <!-- mermaid: add mermaid grafic support -->
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: false, theme: 'default' });
    document.addEventListener("DOMContentLoaded", async () => {
      const mermaidCodes = document.querySelectorAll('pre code.mermaid');
      for (const codeBlock of mermaidCodes) {
        const preTag = codeBlock.parentElement;
        const div = document.createElement('div');
        div.className = 'mermaid';
        div.textContent = codeBlock.textContent;
        preTag.replaceWith(div);
      }
      await mermaid.run({
        querySelector: '.mermaid'
      });
    });
  </script>
  </body>
  </html>

# Input section
# -------------

# input from terminal
InputFromTerminal: true

# input from file (name of file must be specified)
InputFromFile: true
InputFile: prompt-input.txt

# input from localhost (should work on all systems)
InputFromLocalhost: true
InputLocalhostPort: 4242

# Notification section
# --------------------

# notify when prompt processing starts
NotifyPrompt: true
NotifyPromptApplicationMacOS: !!str osascript -e 'display notification "Prompt received ..." with title "gemini-prompt" sound name "Ping"'
NotifyPromptApplicationLinux: !!str notify-send "gemini-prompt" "Prompt received ..." -i info -t 1000
NotifyPromptApplicationWindows: !!str msg * /TIME:2 "gemini-prompt - Prompt received ..."
NotifyPromptApplicationOther:

# notify when prompt processing has finished
NotifyResponse: true
NotifyResponseApplicationMacOS: !!str osascript -e 'display notification "Response received ..." with title "gemini-prompt" sound name "Blow"'
NotifyResponseApplicationLinux: !!str notify-send "gemini-prompt" "Response received ..." -i info -t 1000
NotifyResponseApplicationWindows: !!str msg * /TIME:2 "gemini-prompt - Response received ..."
NotifyResponseApplicationOther:

# History section
# ---------------

# filename schema (possible option: timestamp, prompt)
# timestamp : yyyymmdd-hhmmss.extension
#             e.g. 20250118-134910.html
# prompt    : prefix.[your actual prompt].postfix.extension
#             e.g. [What oceans are thereʔ].20250118-140233.html
#             e.g. 20250118-140233.[What oceans are thereʔ].html
# file schema 'prompt' often allows you to infer file content from filename
HistoryFilenameSchema: prompt

# add timestamp (yyyymmdd-hhmmss) to filename (ensures the uniqueness of the filename)
# this parameters are only useful in conjunction with filename schema 'prompt' 
HistoryFilenameAddPrefix: false
HistoryFilenameAddPostfix: true

# add extension to filename (extensions are often associated with applications)
HistoryFilenameExtensionMarkdown: md
HistoryFilenameExtensionAnsi: ansi
HistoryFilenameExtensionHTML: html

# maximum length of filename (mind your operating system's limitations)
# this parameter is useful in conjunction with filename schema 'prompt' 
HistoryMaxFilenameLength: 200

# General settings section
# ------------------------

# internet proxy url (client -> proxy -> internet)
# 'env:var': obtain proxy setting from environment variable
# 'file:pathname': first line of pathname is proxy setting
# 'pass:api-key': pass contains the proxy setting
# proxy is often set via enviroment variable 'env:HTTPS_PROXY'
# e.g. HTTPS_PROXY=http://USERNAME:PASSWORD@proxy-server.mycorp.com:3128
# do not set anything, if you have a direct internet connection
GeneralInternetProxy: 

# list of MIME type replacements (Gemini currently only supports pdf and text)
MIMETypeReplacements:
- text/x-shellscript = text/plain
- application/json = text/plain
- application/x-ndjson = text/plain
- image/svg+xml = text/plain
